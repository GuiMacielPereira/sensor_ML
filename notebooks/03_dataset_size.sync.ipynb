{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722e4260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Uncomment if running from Google Colab\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive', force_remount=True)\n",
    "%cd gdrive/MyDrive/Masters_Project\n",
    "%pip install -e peratouch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a5d47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run CNN routine for datasets with different sizes\n",
    "# Goal is to analyse how dataset size affects performance\n",
    "from peratouch.data import load_data\n",
    "from peratouch.config import path_five_users_main, path_five_users_first\n",
    "from peratouch.config import path_analysis_results\n",
    "from peratouch.routines import run_network\n",
    "from peratouch.networks import CNN\n",
    "from datetime import date\n",
    "import numpy as np\n",
    "import sklearn\n",
    "\n",
    "n_folds = 5\n",
    "\n",
    "def run_dataset(X, y):\n",
    "    return run_network(CNN, X, y, n_ch=1, n_epochs=100, n_folds=n_folds, n_runs=n_folds, plots=False, n_batches=15, random_resampling=False)\n",
    "\n",
    "Xraw, yraw = load_data(path_five_users_main)\n",
    "Xraw, yraw = sklearn.utils.shuffle(Xraw, yraw)\n",
    "\n",
    "results_dict = {}\n",
    "\n",
    "for n_splits in [80, 40, 20, 10, 5, 4, 3, 2, 1]:         # Splits of raw dataset\n",
    "    print(\"\\n\\n--- Testing new dataset size ---\\n\\n\")\n",
    "\n",
    "    actual_vals, predictions = [], []\n",
    "\n",
    "    if n_splits==1:    # Case of entire dataset\n",
    "        X = Xraw\n",
    "        y = yraw\n",
    "\n",
    "        actual, preds = run_dataset(X, y)\n",
    "\n",
    "        actual_vals.extend(actual)\n",
    "        predictions.extend(preds)\n",
    "\n",
    "    else:    \n",
    "      kf = sklearn.model_selection.KFold(n_splits)       # No shuffling\n",
    "      for i, (_, data_idx) in enumerate(kf.split(Xraw)):\n",
    "          print(\"\\n-- New splitting of dataset --\\n\")\n",
    "\n",
    "          X = Xraw[data_idx]\n",
    "          y = yraw[data_idx]\n",
    "          \n",
    "          actual, preds = run_dataset(X, y)\n",
    "\n",
    "          actual_vals.extend(actual)\n",
    "          predictions.extend(preds)\n",
    "\n",
    "    # NOTE: Hardcoded value of 0.85 below is the split between training and validation sets\n",
    "    tr_size = int(len(X)*(n_folds-1)/n_folds * 0.85)         # Training size is size of all folds except one \n",
    "    results_dict[str(tr_size)] = (actual_vals, predictions)\n",
    "    filename = str(path_analysis_results / f\"dataset_size_{date.today()}.npz\") \n",
    "    np.savez(filename, **results_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb37eb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads data from stored dict\n",
    "stored_results = np.load(filename)\n",
    "print(\"len of raw data: \", len(Xraw))\n",
    "for key in stored_results:\n",
    "    print(key, \" : \", len(results_dict[key][0]))\n",
    "    print(sklearn.metrics.classification_report(*results_dict[key]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
