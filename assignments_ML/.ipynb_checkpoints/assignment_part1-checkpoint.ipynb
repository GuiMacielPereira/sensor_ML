{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IV6UGwSdcYHf"
   },
   "source": [
    "Enter your username (used for marking):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "rybkkNgScYHi"
   },
   "outputs": [],
   "source": [
    "username = 'myusername'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j0FiyuFscYHl"
   },
   "source": [
    "# COM4509/6509 Coursework Part 1\n",
    "\n",
    "Hello,\n",
    "This is the first of the two parts. Each part accounts for 50\\% of the overall coursework mark.\n",
    "\n",
    "### What to submit\n",
    "\n",
    "- You need to submit **two jupyter notebooks** (not zipped together), named:\n",
    "\n",
    "```\n",
    "assignment_part1_[username].ipynb\n",
    "assignment_part2_[username].ipynb\n",
    "```\n",
    "\n",
    "replacing `[username]` with your username, e.g. `abc18de`.\n",
    "\n",
    "- Please do not upload the data files used in this Notebook. We just want the two python notebooks.\n",
    "\n",
    "### Assessment Criteria \n",
    "\n",
    "- The marks are indicated for each part: You'll get marks for correct code that does what is asked and gets the right answer. These contribute 45.\n",
    "- There are also 5 marks for \"Code quality\" (includes both readability and efficiency).\n",
    "\n",
    "### Late submissions\n",
    "\n",
    "We follow the department's guidelines about late submissions, Undergraduate [handbook link](https://sites.google.com/sheffield.ac.uk/comughandbook/your-study/assessment/late-submission). PGT [handbook link](https://sites.google.com/sheffield.ac.uk/compgtstudenthandbook/home/your-study/assessment/late-submission).\n",
    "\n",
    "### Use of unfair means\n",
    "\"Any form of unfair means is treated as a serious academic offence and action may be taken under the Discipline Regulations.\" (from the students Handbook).\n",
    "\n",
    "# A dataset of air quality\n",
    "\n",
    "We are going to use a dataset of air pollution measurements in Beijing archived by the UCI. To read about the dataset visit the [UCI page](https://archive.ics.uci.edu/ml/datasets/Beijing+Multi-Site+Air-Quality+Data).\n",
    "\n",
    "We are going to:\n",
    "  1. Preprocess the data\n",
    "  2. Build our own Lasso-regression\n",
    "  3. Use sklearn's tools to perform regression on the data\n",
    "  \n",
    "Let's get started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "NWxSewmucYHp"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import linear_model\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import urllib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WWSGSGFfcYHs"
   },
   "source": [
    "We will be trying to predict the pollution (PM2.5) using:\n",
    "- temperature ('TEMP')\n",
    "- pressure ('PRES')\n",
    "- dew-point temperature ('DEWP')\n",
    "- precipitation ('RAIN')\n",
    "- wind direction ('wd')\n",
    "- wind speed ('WSPM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "QujKMUPtcYHu"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./data.csv', <http.client.HTTPMessage at 0x7fa3f859bca0>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urllib.request.urlretrieve('https://drive.google.com/uc?id=1m1g4Xn1wMAGV_EU0Nh1HTI1ogA3-tqJk&export=download', './data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "2qOVpf72cYHv"
   },
   "outputs": [],
   "source": [
    "raw_df = pd.read_csv('data.csv',index_col='No')\n",
    "\n",
    "#put the columns in a useful order\n",
    "raw_df = raw_df[['PM2.5', 'year', 'month', 'day', 'hour', 'PM10', 'SO2', 'NO2', 'CO',\n",
    "       'O3', 'TEMP', 'PRES', 'DEWP', 'RAIN', 'wd', 'WSPM', 'station']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oQKg3AFPcYHx"
   },
   "source": [
    "Some of the records are missing. We need to handle that before we can easily use the data with most ML tools.\n",
    "\n",
    "### Question 1: Removing missing data [1 mark]\n",
    "\n",
    "We are going to handle this by dropping those rows which have a NaN in one of these columns: ['PM2.5','hour','TEMP','PRES','DEWP','RAIN','wd','WSPM'].\n",
    "\n",
    "Save the result in `nonull_df`.\n",
    "\n",
    "We can use `df.dropna` to do this. Pandas documentation on this method is [here](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.dropna.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "k0cYywfFcYHy"
   },
   "outputs": [],
   "source": [
    "#Put answer here\n",
    "nonull_df = raw_df.dropna(subset=['PM2.5','hour','TEMP','PRES','DEWP','RAIN','wd','WSPM'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UNruaXp6cYHz"
   },
   "source": [
    "To check you've done it correctly, you could use `clean_df.isnull().sum()` to confirm that there are no NaN rows in the columns we're interested in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "zPWKkfzHcYH0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PM2.5        0\n",
       "year         0\n",
       "month        0\n",
       "day          0\n",
       "hour         0\n",
       "PM10        17\n",
       "SO2        242\n",
       "NO2        331\n",
       "CO         866\n",
       "O3         685\n",
       "TEMP         0\n",
       "PRES         0\n",
       "DEWP         0\n",
       "RAIN         0\n",
       "wd           0\n",
       "WSPM         0\n",
       "station      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nonull_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8ePRfNOEcYH1"
   },
   "source": [
    "### Question 2: Removing unwanted columns [1 mark]\n",
    "\n",
    "Let's remove the columns we're not going to be using. We can use `nonull_df.drop(list_of_column_names, axis=1)` to do this. We will drop: ['year','month','day','PM10','SO2','NO2','CO','O3','station'].\n",
    "\n",
    "Again, feel free to check if it's worked with `clean_df.isnull().sum()`, for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "edYxDc5NcYH1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PM2.5    0\n",
       "hour     0\n",
       "TEMP     0\n",
       "PRES     0\n",
       "DEWP     0\n",
       "RAIN     0\n",
       "wd       0\n",
       "WSPM     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_df = nonull_df.drop(['year','month','day','PM10','SO2','NO2','CO','O3','station'], axis=1)\n",
    "clean_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "0FGsOUWbcYH2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(34284, 8)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#there should be 34284 rows left in your dataframe, and 8 columns (note, this was corrected on 7/11/22)\n",
    "clean_df.shape #=(34284, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UoskxkwycYH3"
   },
   "source": [
    "### Question 3: Splitting the dataset [2 marks]\n",
    "\n",
    "Before designing any machine learning model, we need to set aside the test data. We will use the remaining training data for fitting the model. **It is important to remember that the test data has to be set aside before preprocessing**.\n",
    "\n",
    "Any preprocessing that you do has to be done only on the training data and several key statistics need to be saved for the test stage. Separating the dataset into training and test before any preprocessing has happened help us to recreate the real world scenario where we will deploy our system and for which the data will come without any preprocessing.\n",
    "\n",
    "Later we will be performing a grid search to select parameter values. To do this we'll do cross-validation, but rather than split the data into training and validation here we'll split it later. So for now we'll just split into:\n",
    "\n",
    "- The training (and validation) set will have 85% of the total observations, \n",
    "- The test set, the remaining 15%.\n",
    "\n",
    "To avoid unwanted correlations connecting the training and test, we will split these by time. So:\n",
    "\n",
    "- Take the first 85% of the rows from clean_df and put them in train_df, take the remaining 15% of the rows and put them in test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "NjbrkiVucYH5"
   },
   "outputs": [],
   "source": [
    "#Put answer here\n",
    "# Split the data without reshuffling because otherwise test data samples could be \n",
    "# very similar to train data particularly at a given hour\n",
    "\n",
    "split = 0.85\n",
    "idxSplit = int(split * len(clean_df))\n",
    "\n",
    "train_df = clean_df.iloc[:idxSplit, :]\n",
    "test_df = clean_df.iloc[idxSplit:, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hn-utn5pcYH6"
   },
   "source": [
    "To check the sizes are correct, we can use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "lYolbW7RcYH6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8499883327499709, 0.15001166725002918)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_df)/len(clean_df),len(test_df)/len(clean_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xMvSVU3-cYH7"
   },
   "source": [
    "# Detour: Lasso Regression\n",
    "\n",
    "Later we will use the sklearn toolkit, but in this section **you will develop your own code** to do the Lasso regression.\n",
    "\n",
    "### Ordinary Least Squares Regression\n",
    "\n",
    "First, let's just perform normal linear regression.\n",
    "\n",
    "We'll use a toy design matrix & labels to use to check our code works. We'll also specify a weight vector too, for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "jRBjUyWgcYH7"
   },
   "outputs": [],
   "source": [
    "X = np.array([[0.0,0],[1,3],[2.2,3]])\n",
    "y = np.array([0.0,1,2])\n",
    "w = np.array([1.0,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ou21m-UNcYH7"
   },
   "source": [
    "### Question 4: Prediction Function [2 marks]\n",
    "\n",
    "The first task is to write a function to make predictions. Can you complete this function for linear regression, i.e. the predictions for all our points $f(X,\\mathbf{w}) = X \\mathbf{w}$. [corrected: 7/11/22, don't need $X$ tranposed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "r2jQlwn0cYH8"
   },
   "outputs": [],
   "source": [
    "def prediction(X,w):\n",
    "    return X @ w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "gEKkPNjgcYH8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#You can use this code to check you've written the right function\n",
    "np.all(prediction(X,w)==np.array([0. , 7. , 8.2])) #Should return 'True'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wha2MzM7cYH9"
   },
   "source": [
    "### Question 5: Objective Function [4 marks]\n",
    "\n",
    "Now we need to write a function that returns the 'error'.\n",
    "We'll just do normal Ordinary Least Squares with linear regression, so if you remember the cost function for that is:\n",
    "\n",
    "$$E = \\sum_{i=1}^N \\big(y_i-f(\\mathbf{x}_i,\\mathbf{w}) \\big)^2$$\n",
    "\n",
    "Where $E$ is the error, $N$ the number of points, $y_i$ is one of the labels, $\\mathbf{x}_i$ is the input for that label, $\\mathbf{w}$ is the weight vector. $f$ is the prediction function you've already written. Or feel free to substitute in $\\mathbf{x}_i^\\top \\mathbf{w}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "LKWQ2Y41cYH9"
   },
   "outputs": [],
   "source": [
    "def objective(X,y,w):\n",
    "    \"\"\"\n",
    "    Computes the sum squared error (for us to perform OLS linear regression).\n",
    "    \"\"\"\n",
    "    return np.sum(np.square(y - X @ w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "OBo1zJeicYH-"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#You can use this code to check you've written the right function\n",
    "objective(X,y,w)==74.44"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9BhH_-zocYH-"
   },
   "source": [
    "### Question 6: Objective Function Gradient [4 marks]\n",
    "\n",
    "Now you need to find the derivative of the objective wrt the parameter vector. You've already done this in lectures, so remember the gradient of the error function (for linear regression) is:\n",
    "\n",
    "$$\\frac{\\partial E}{\\partial \\mathbf{w}} = 2 X^\\top X \\mathbf{w} - 2 X^\\top y$$\n",
    "\n",
    "Add code to do this here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "O2TD0-v9cYH-"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([39.28, 73.2 ])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def objective_derivative(X,y,w):\n",
    "    \"\"\"\n",
    "    Computes the derivative of the sum squared error, wrt the parameters.\n",
    "    \"\"\"\n",
    "    return 2 * X.T @ X @ w - 2 * X.T @ y \n",
    "objective_derivative(X,y,w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tgvPQtC0cYH_"
   },
   "source": [
    "To check your gradients are correct, we can estimate the gradient numerically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "9MV54TG3cYH_"
   },
   "outputs": [],
   "source": [
    "def numerical_objective_derivative(X,y,w):\n",
    "    \"\"\"\n",
    "    Computes a numerical approximation to the derivative of the sum squared error, wrt the parameters.\n",
    "    \"\"\"\n",
    "    g = np.zeros_like(w)\n",
    "    for i,wi in enumerate(w):\n",
    "        d = np.zeros_like(w)\n",
    "        d[i]=1e-6\n",
    "        g[i] = (objective(X,y,w+d)-objective(X,y,w-d))/2e-6\n",
    "    return g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "42SCGt-0cYIA"
   },
   "source": [
    "The two gradient vectors should be approximately equal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "zuRzN4-xcYIA"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([39.28, 73.2 ]), array([39.28, 73.2 ]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "objective_derivative(X,y,w),numerical_objective_derivative(X,y,w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-58158ZjcYIA"
   },
   "source": [
    "### Question 7: Optimise $\\mathbf{w}$ to minimise the objective [4 marks]\n",
    "\n",
    "Now you need to use the gradient function you've written to maximise $\\mathbf{w}$ using gradient descent.\n",
    "Start with a sensible choice of w. You'll need to loop lots of times (e.g. 1000). At each iteration: compute the gradient and subtract the scaled gradient from the w parameter (you'll need to scale it by the learning rate, of e.g. 0.01)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "eq9GtHEAcYIB"
   },
   "outputs": [],
   "source": [
    "def optimise_parameters(X,y,startw):\n",
    "    \"\"\"\n",
    "    Returns the w that minimises the objective.\n",
    "    \"\"\"\n",
    "    lr = 1e-2\n",
    "    w = startw\n",
    "    for i in range(1000):\n",
    "        \n",
    "        loss = objective(X, y, w)\n",
    "        \n",
    "        if i%100==0: print(f\"Iteration {i}, Loss: {loss:.3e}\")\n",
    "        \n",
    "        w -= lr * objective_derivative(X, y, w)\n",
    "        \n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "tmbmC6B5cYIB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Loss: 7.444e+01\n",
      "Iteration 100, Loss: 3.716e-02\n",
      "Iteration 200, Loss: 3.960e-03\n",
      "Iteration 300, Loss: 4.220e-04\n",
      "Iteration 400, Loss: 4.498e-05\n",
      "Iteration 500, Loss: 4.793e-06\n",
      "Iteration 600, Loss: 5.108e-07\n",
      "Iteration 700, Loss: 5.443e-08\n",
      "Iteration 800, Loss: 5.801e-09\n",
      "Iteration 900, Loss: 6.182e-10\n",
      "[0.8333238 0.0555608]\n"
     ]
    }
   ],
   "source": [
    "bestw = optimise_parameters(X,y,w)\n",
    "print(bestw) #print our solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TG713sjPcYIB"
   },
   "source": [
    "Let's compare this to the answer provided by sklearn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "0qzeBjibcYIC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.83333333 0.05555556]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "clf = linear_model.LinearRegression(fit_intercept=False)\n",
    "clf.fit(X,y)\n",
    "print(clf.coef_) #matches the value of w we found above, hopefully!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pn_4IihFcYIC"
   },
   "source": [
    "## Lasso Regression\n",
    "\n",
    "### Question 8: New Objective Function [3 marks]\n",
    "\n",
    "We're now going to regularise the regression using $L_1$ regularisation - i.e. Lasso regression.\n",
    "\n",
    "We need a **new objective function**:\n",
    "\n",
    "$$E = \\frac{1}{2N}\\sum_{i=1}^N \\big(y_i-f(\\mathbf{x}_i,\\mathbf{w}) \\big)^2 + \\alpha \\sum_{j=1}^P |w_j|$$\n",
    "\n",
    "This is similar to before (but the first term is now half the mean squared error, rather than the sum squared error). The second term is the L1 regularisation term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "dl24ORnocYID"
   },
   "outputs": [],
   "source": [
    "def objective_lasso(X,y,w,alpha):\n",
    "    \"\"\"\n",
    "    Computes half the mean squared error, with an additional L1 regularising term. alpha controls the level of regularisation.\n",
    "    \"\"\"\n",
    "    return 0.5 * np.mean(np.square(y - X @ w)) + alpha * np.sum(np.abs(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FL_qwDiocYID"
   },
   "source": [
    "### Question 9: The gradient of the lasso regression objective [3 marks]\n",
    "\n",
    "The tricky bit the derivative of the objective.\n",
    "\n",
    "The first part is similar to before. So, with the regularising term, the derivative is:\n",
    "$$\\frac{\\partial E}{\\partial \\mathbf{w}} = \\frac{1}{N}(X^\\top X \\mathbf{w} - X^\\top y) + \\alpha\\;\\text{sign}(\\mathbf{w})$$\n",
    "\n",
    "where $\\text{sign}(\\mathbf{w})$ returns a vector of the same shape as $\\mathbf{w}$ with +1 if the element is positive and -1 if it's negative. The `np.sign` method does this for you.\n",
    "\n",
    "Have a think about why this is (think about what differentiating the 'absolute' function $|w_j|$ involves - think about what happens when it's positive vs when it's negative. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "0cCZM2kDcYID"
   },
   "outputs": [],
   "source": [
    "def objective_lasso_derivative(X,y,w,alpha):\n",
    "    \"\"\"\n",
    "    Returns the derivative of the Lasso objective function.\n",
    "    \"\"\"\n",
    "    N = len(y)\n",
    "    return 1/N * (X.T @ X @ w - X.T @ y) + alpha * np.sign(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F8BaS8g5cYIE"
   },
   "source": [
    "We can check it again, numerically. The two pairs of parameters should be the same:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "V0aMgXBEcYIE"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.09999823, 0.10000097]), array([0.09999823, 0.10000097]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def numerical_objective_lasso_derivative(X,y,w,alpha):\n",
    "    \"\"\"\n",
    "    This finds a numerical approximation to the true gradient\n",
    "    \"\"\"\n",
    "    g = np.zeros_like(w)\n",
    "    for i,wi in enumerate(w):\n",
    "        d = np.zeros_like(w)\n",
    "        d[i]=1e-6\n",
    "        g[i] = (objective_lasso(X,y,w+d,alpha)-objective_lasso(X,y,w-d,alpha))/2e-6\n",
    "    return g\n",
    "\n",
    "objective_lasso_derivative(X,y,w,0.1),numerical_objective_lasso_derivative(X,y,w,0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B-m4LjUHcYIE"
   },
   "source": [
    "### Question 10: Optimise $\\mathbf{w}$ to minimise the Lasso objective [2 marks]\n",
    "\n",
    "As before we need to optimise to find the optimum value of $\\mathbf{w}$, for this Lasso objective.\n",
    "You'll need to loop lots of times (e.g. 5000). Start with a sensible choice of w. At each iteration: compute the gradient and subtract the scaled gradient from the w parameter (you'll need to scale it by the learning rate, of e.g. 0.05)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "QpsfOgxFcYIF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Loss: 8.889e-02\n",
      "Iteration 100, Loss: 8.417e-02\n",
      "Iteration 200, Loss: 8.362e-02\n",
      "Iteration 300, Loss: 8.353e-02\n",
      "Iteration 400, Loss: 8.352e-02\n",
      "Iteration 500, Loss: 8.352e-02\n",
      "Iteration 600, Loss: 8.352e-02\n",
      "Iteration 700, Loss: 8.352e-02\n",
      "Iteration 800, Loss: 8.352e-02\n",
      "Iteration 900, Loss: 8.352e-02\n",
      "Iteration 1000, Loss: 8.352e-02\n",
      "Iteration 1100, Loss: 8.352e-02\n",
      "Iteration 1200, Loss: 8.352e-02\n",
      "Iteration 1300, Loss: 8.352e-02\n",
      "Iteration 1400, Loss: 8.352e-02\n",
      "Iteration 1500, Loss: 8.352e-02\n",
      "Iteration 1600, Loss: 8.352e-02\n",
      "Iteration 1700, Loss: 8.352e-02\n",
      "Iteration 1800, Loss: 8.352e-02\n",
      "Iteration 1900, Loss: 8.352e-02\n",
      "Iteration 2000, Loss: 8.352e-02\n",
      "Iteration 2100, Loss: 8.352e-02\n",
      "Iteration 2200, Loss: 8.352e-02\n",
      "Iteration 2300, Loss: 8.352e-02\n",
      "Iteration 2400, Loss: 8.352e-02\n",
      "Iteration 2500, Loss: 8.352e-02\n",
      "Iteration 2600, Loss: 8.352e-02\n",
      "Iteration 2700, Loss: 8.352e-02\n",
      "Iteration 2800, Loss: 8.352e-02\n",
      "Iteration 2900, Loss: 8.352e-02\n",
      "Iteration 3000, Loss: 8.352e-02\n",
      "Iteration 3100, Loss: 8.352e-02\n",
      "Iteration 3200, Loss: 8.352e-02\n",
      "Iteration 3300, Loss: 8.352e-02\n",
      "Iteration 3400, Loss: 8.352e-02\n",
      "Iteration 3500, Loss: 8.352e-02\n",
      "Iteration 3600, Loss: 8.352e-02\n",
      "Iteration 3700, Loss: 8.352e-02\n",
      "Iteration 3800, Loss: 8.352e-02\n",
      "Iteration 3900, Loss: 8.352e-02\n",
      "Iteration 4000, Loss: 8.352e-02\n",
      "Iteration 4100, Loss: 8.352e-02\n",
      "Iteration 4200, Loss: 8.352e-02\n",
      "Iteration 4300, Loss: 8.352e-02\n",
      "Iteration 4400, Loss: 8.352e-02\n",
      "Iteration 4500, Loss: 8.352e-02\n",
      "Iteration 4600, Loss: 8.352e-02\n",
      "Iteration 4700, Loss: 8.352e-02\n",
      "Iteration 4800, Loss: 8.352e-02\n",
      "Iteration 4900, Loss: 8.352e-02\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.63888889, 0.14259259])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def optimise_parameters_lasso(X,y,startw):\n",
    "    \"\"\"\n",
    "    Returns the w that minimises the Lasso objective.\n",
    "    \"\"\"\n",
    "    #Put answer here\n",
    "    lr = 5e-2\n",
    "    w = startw\n",
    "    alpha = 0.1\n",
    "    \n",
    "    for i in range(5000):\n",
    "        \n",
    "        loss = objective_lasso(X, y, w, alpha)\n",
    "        \n",
    "        if i%100==0: print(f\"Iteration {i}, Loss: {loss:.3e}\")\n",
    "        \n",
    "        w -= lr * objective_lasso_derivative(X, y, w, alpha)\n",
    "        \n",
    "    return w\n",
    "\n",
    "optimise_parameters_lasso(X,y,w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GK2UPtJHcYIG"
   },
   "source": [
    "We can check against the sklearn method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "APPYBmk_cYIG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.63931263 0.1423666 ]\n"
     ]
    }
   ],
   "source": [
    "clf = linear_model.Lasso(alpha=0.1,fit_intercept=False)\n",
    "clf.fit(X,y)\n",
    "print(clf.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "438cqaLtcYIH"
   },
   "source": [
    "The above result should approximately match the one you computed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D5c76jIVcYIH"
   },
   "source": [
    "# Back to air pollution\n",
    "\n",
    "### Question 11: One-hot-encoding [4 marks]\n",
    "\n",
    "One of the columns isn't numerical, but instead is a string type: The wind direction. The best way to deal with this is one-hot-encoding.\n",
    "\n",
    "pandas has a tool for doing this: `pd.get_dummies(series, prefix='prefix_to_use')`. In our example the series is: `clean_df.wd`.\n",
    "\n",
    "You'll need to:\n",
    "1. Make the one-hot encoding table using the code above.\n",
    "2. Delete the `wd` column from our table (hint: you did this earlier for other columns).\n",
    "3. Join the one hot data to the table. To do this use something like `dataframe1.join(dataframe2)`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "4hUiiPE2cYII"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PM2.5</th>\n",
       "      <th>hour</th>\n",
       "      <th>TEMP</th>\n",
       "      <th>PRES</th>\n",
       "      <th>DEWP</th>\n",
       "      <th>RAIN</th>\n",
       "      <th>WSPM</th>\n",
       "      <th>E</th>\n",
       "      <th>ENE</th>\n",
       "      <th>ESE</th>\n",
       "      <th>...</th>\n",
       "      <th>NNW</th>\n",
       "      <th>NW</th>\n",
       "      <th>S</th>\n",
       "      <th>SE</th>\n",
       "      <th>SSE</th>\n",
       "      <th>SSW</th>\n",
       "      <th>SW</th>\n",
       "      <th>W</th>\n",
       "      <th>WNW</th>\n",
       "      <th>WSW</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>No</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>1021.9</td>\n",
       "      <td>-19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>1022.4</td>\n",
       "      <td>-19.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8.0</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.6</td>\n",
       "      <td>1022.6</td>\n",
       "      <td>-19.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8.0</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.7</td>\n",
       "      <td>1023.5</td>\n",
       "      <td>-20.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8.0</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.9</td>\n",
       "      <td>1024.1</td>\n",
       "      <td>-21.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29760</th>\n",
       "      <td>138.0</td>\n",
       "      <td>23</td>\n",
       "      <td>28.2</td>\n",
       "      <td>997.0</td>\n",
       "      <td>24.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29761</th>\n",
       "      <td>145.0</td>\n",
       "      <td>0</td>\n",
       "      <td>27.7</td>\n",
       "      <td>997.1</td>\n",
       "      <td>24.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29762</th>\n",
       "      <td>139.0</td>\n",
       "      <td>1</td>\n",
       "      <td>26.9</td>\n",
       "      <td>996.6</td>\n",
       "      <td>24.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29763</th>\n",
       "      <td>128.0</td>\n",
       "      <td>2</td>\n",
       "      <td>26.6</td>\n",
       "      <td>996.3</td>\n",
       "      <td>24.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29764</th>\n",
       "      <td>116.0</td>\n",
       "      <td>3</td>\n",
       "      <td>26.4</td>\n",
       "      <td>996.2</td>\n",
       "      <td>24.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>29141 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       PM2.5  hour  TEMP    PRES  DEWP  RAIN  WSPM  E  ENE  ESE  ...  NNW  NW  \\\n",
       "No                                                               ...            \n",
       "1        9.0     0   0.3  1021.9 -19.0   0.0   2.0  0    0    0  ...    0   0   \n",
       "2       11.0     1  -0.1  1022.4 -19.3   0.0   4.4  0    0    0  ...    0   0   \n",
       "3        8.0     2  -0.6  1022.6 -19.7   0.0   4.7  0    0    0  ...    0   0   \n",
       "4        8.0     3  -0.7  1023.5 -20.9   0.0   2.6  0    0    0  ...    0   1   \n",
       "5        8.0     4  -0.9  1024.1 -21.7   0.0   2.5  0    0    0  ...    0   0   \n",
       "...      ...   ...   ...     ...   ...   ...   ... ..  ...  ...  ...  ...  ..   \n",
       "29760  138.0    23  28.2   997.0  24.2   0.0   1.0  0    0    0  ...    0   0   \n",
       "29761  145.0     0  27.7   997.1  24.3   0.0   0.9  0    0    0  ...    0   0   \n",
       "29762  139.0     1  26.9   996.6  24.6   0.0   0.8  1    0    0  ...    0   0   \n",
       "29763  128.0     2  26.6   996.3  24.8   0.0   0.4  0    0    0  ...    0   0   \n",
       "29764  116.0     3  26.4   996.2  24.6   0.0   0.9  1    0    0  ...    0   0   \n",
       "\n",
       "       S  SE  SSE  SSW  SW  W  WNW  WSW  \n",
       "No                                       \n",
       "1      0   0    0    0   0  0    1    0  \n",
       "2      0   0    0    0   0  0    1    0  \n",
       "3      0   0    0    0   0  0    1    0  \n",
       "4      0   0    0    0   0  0    0    0  \n",
       "5      0   0    0    0   0  0    1    0  \n",
       "...   ..  ..  ...  ...  .. ..  ...  ...  \n",
       "29760  0   0    1    0   0  0    0    0  \n",
       "29761  0   0    0    1   0  0    0    0  \n",
       "29762  0   0    0    0   0  0    0    0  \n",
       "29763  0   0    0    0   0  0    0    0  \n",
       "29764  0   0    0    0   0  0    0    0  \n",
       "\n",
       "[29141 rows x 23 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def add_wd_onehot(df):\n",
    "    \"\"\"Add new one-hot encoding set of columns, removes the old column it's made from. Returns new dataframe.\"\"\"\n",
    "    \n",
    "    # Get one hot encoding of columns 'vehicleType'\n",
    "    table = pd.get_dummies(df[\"wd\"])\n",
    "\n",
    "    # Drop column as it is now encoded\n",
    "    encoded_df = df.drop([\"wd\"], axis=1)\n",
    "\n",
    "    # Join the encoded df\n",
    "    encoded_df = encoded_df.join(table)\n",
    "\n",
    "    return encoded_df\n",
    "\n",
    "#you could use this code to see if it's worked?\n",
    "train_df_wdencoded = add_wd_onehot(train_df)\n",
    "train_df_wdencoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gR9m46ghcYIJ"
   },
   "source": [
    "### Question 12: Standardise the data [3 marks]\n",
    "[note: updated from term 'Normalise' to 'Standardise' on 7/11/22. For clarity, I want the mean to be zero and the standard deviation to be one].\n",
    "\n",
    "Now we need to standardise [edit: corrected] the data.\n",
    "\n",
    "You could manipulate just some columns by using, for example: `df.iloc[:,1:]` - this returns a dataframe that consists of all but the first column.\n",
    "\n",
    "Feel free to use either tools from `sklearn.preprocessing` or standardise [edit: corrected on 7/11/22] it by, for example, using the mean and the standard deviation of the columns by calling `some_dataframe.mean()` or `some_dataframe.std()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "ylCMFcVccYIJ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(PM2.5    82.432774\n",
       " hour      1.000000\n",
       " TEMP      1.000000\n",
       " PRES      1.000000\n",
       " DEWP      1.000000\n",
       " RAIN      1.000000\n",
       " WSPM      1.000000\n",
       " E         1.000000\n",
       " ENE       1.000000\n",
       " ESE       1.000000\n",
       " N         1.000000\n",
       " NE        1.000000\n",
       " NNE       1.000000\n",
       " NNW       1.000000\n",
       " NW        1.000000\n",
       " S         1.000000\n",
       " SE        1.000000\n",
       " SSE       1.000000\n",
       " SSW       1.000000\n",
       " SW        1.000000\n",
       " W         1.000000\n",
       " WNW       1.000000\n",
       " WSW       1.000000\n",
       " dtype: float64,\n",
       " PM2.5    103.261131\n",
       " hour       0.997833\n",
       " TEMP       1.044087\n",
       " PRES       0.942879\n",
       " DEWP       1.051096\n",
       " RAIN       1.128614\n",
       " WSPM       0.891056\n",
       " E          1.030425\n",
       " ENE        1.172341\n",
       " ESE        1.101838\n",
       " N          1.137986\n",
       " NE         1.392286\n",
       " NNE        1.137867\n",
       " NNW        1.186881\n",
       " NW         1.027553\n",
       " S          0.914140\n",
       " SE         1.297810\n",
       " SSE        1.047106\n",
       " SSW        0.711625\n",
       " SW         0.934248\n",
       " W          0.536410\n",
       " WNW        0.522378\n",
       " WSW        0.686292\n",
       " dtype: float64)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def standardise(df, train_df):\n",
    "    \"\"\"\n",
    "    Returns a new dataframe in which all but the PM2.5 columns are standardised (i.e. have a mean of zero and standard deviation of 1)\n",
    "    [note: the function name used to be 'normalise' but was modified for clarity]\n",
    "\n",
    "    [addition: 7/11/22\n",
    "    Think about if you want to standardise using the *training* data's mean and standard deviation (for the test data).\n",
    "    \"\"\"\n",
    "    \n",
    "    # Use training set to standardize df, both for training and test\n",
    "    stand_df = df.copy()\n",
    "    stand_df.iloc[:, 1:] -= train_df.iloc[:,1:].mean()\n",
    "    stand_df.iloc[:, 1:] /= train_df.iloc[:, 1:].std()\n",
    "    \n",
    "    return stand_df\n",
    "\n",
    "train_df_preprocessed = standardise(add_wd_onehot(train_df), add_wd_onehot(train_df))\n",
    "\n",
    "# I believe the preprocessing on \n",
    "test_df_preprocessed = standardise(add_wd_onehot(test_df), add_wd_onehot(train_df))\n",
    "\n",
    "# Check first column is unchanged and rest of columns are standardized'\n",
    "train_df_preprocessed.std(), test_df_preprocessed.std()\n",
    "\n",
    "# And we check training data is standardized to 1 while test data has more variance around 1\n",
    "# This is becuase the test data preprocessing is using the training data means and std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7cUc5TFLcYIJ"
   },
   "source": [
    "Here we put the training and test inputs (X) and outputs (y) into four variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "9reEdwwjcYIJ"
   },
   "outputs": [],
   "source": [
    "X = train_df_preprocessed.iloc[:,1:].to_numpy()\n",
    "y = train_df_preprocessed.iloc[:,0].to_numpy()\n",
    "\n",
    "Xtest = test_df_preprocessed.iloc[:,1:].to_numpy()\n",
    "ytest = test_df_preprocessed.iloc[:,0].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Cbnx-YWcYIK"
   },
   "source": [
    "We can use the same code we wrote before, using the Lasso from sklearn to fit the data. Here we'll turn fit_intercept on, as we've not added a '1's column to our design matrix.\n",
    "\n",
    "So feel free to use:\n",
    "```\n",
    "clf = linear_model.Lasso(alpha=0.1,fit_intercept=True)\n",
    "clf.fit(X,y)\n",
    "```\n",
    "\n",
    "### Question 13: Finding the RMSE of the Lasso regressor predictions [2 marks]\n",
    "\n",
    "Next compute the **RMSE** of the predictions for (a) the training data and (b) the test data.\n",
    "The RMSE (root mean squared error) could be computed, for example with:\n",
    "\n",
    "```np.sqrt(np.mean((predicted_values-true_values)**2))```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RMSE(ypred, y):\n",
    "    return np.sqrt(np.mean((ypred - y)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "m6HCR6qWcYIK"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(72.40451940025356, 87.60477726176146)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Specify model\n",
    "clf = linear_model.Lasso(alpha=0.1, fit_intercept=True)\n",
    "\n",
    "# Run optimization to find minimum\n",
    "clf.fit(X, y)\n",
    "\n",
    "# Make prediction using found minima\n",
    "ypred_l_train = clf.predict(X)\n",
    "ypred_l_test = clf.predict(Xtest)\n",
    "\n",
    "# Errors\n",
    "rmse_lasso_train = RMSE(ypred_l_train, y)\n",
    "rmse_lasso_test = RMSE(ypred_l_test, ytest) \n",
    "\n",
    "# RMSE uses the same formula as the std but instead of comparing values to\n",
    "# the mean, the errors are computed on a point by point basis\n",
    "rmse_lasso_train, rmse_lasso_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QzqNV3ticYIL"
   },
   "source": [
    "We can compare this to the standard deviation of the data, we should do better than that!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "NYXvIHwMcYIL"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(82.43136001277581, 103.25109103178704)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(y), np.std(ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qMNAk_HrcYIL"
   },
   "source": [
    "### Question 14: Random Forest [8 marks]\n",
    "\n",
    "The final step is to use a random forest regressor.\n",
    "\n",
    "If we use the default random forest regressor, we find we get considerable over-fitting. So we need to explore different parameters. We will use a cross-validated grid search over the parameters:\n",
    "\n",
    "- max_features: The number of features to consider when looking for the best split (i.e. controls subsampling), *from 1 to the number of features* in 4 steps (e.g. use `np.linspace`)\n",
    "- n_estimators: The number of trees in the forest, from 10 to 100 in 4 steps.\n",
    "- max_samples : the number of samples to draw from to train each base estimator, from 0.1 to 0.9 in 4 steps.\n",
    "\n",
    "We will use `GridSearchCV`.\n",
    "\n",
    "Have a look at the documentation for this, the three parameters we need to specify are:\n",
    "\n",
    "- the 'estimator': an **INSTANCE** of RandomForestRegressor.\n",
    "- param_grid: a **DICTIONARY**, each item is the title of the parameter, and equals an array of the values we need to test. For exmaple, one of the items might be `{'max_samples': np.linspace(0.1, 0.9, 5)}`.\n",
    "- You'll need to think carefully how to make the lists for the `max_features` and `n_estimators` as these both need to be (positive) integers. E.g. use `.astype(int)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "vObtrvKxcYIM"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(estimator=RandomForestRegressor(),\n",
       "             param_grid={&#x27;max_features&#x27;: array([ 1,  8, 15, 22]),\n",
       "                         &#x27;max_samples&#x27;: array([0.1       , 0.36666667, 0.63333333, 0.9       ]),\n",
       "                         &#x27;n_estimators&#x27;: array([ 10,  40,  70, 100])})</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(estimator=RandomForestRegressor(),\n",
       "             param_grid={&#x27;max_features&#x27;: array([ 1,  8, 15, 22]),\n",
       "                         &#x27;max_samples&#x27;: array([0.1       , 0.36666667, 0.63333333, 0.9       ]),\n",
       "                         &#x27;n_estimators&#x27;: array([ 10,  40,  70, 100])})</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor()</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(estimator=RandomForestRegressor(),\n",
       "             param_grid={'max_features': array([ 1,  8, 15, 22]),\n",
       "                         'max_samples': array([0.1       , 0.36666667, 0.63333333, 0.9       ]),\n",
       "                         'n_estimators': array([ 10,  40,  70, 100])})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ncols = len(X[0])\n",
    "assert len(Xtest[0])==Ncols, \"Number of columns for Xtest and X need to be the same\"\n",
    "\n",
    "#1. Create a grid of parameter values for n_estimators, max_features and max_samples,\n",
    "grid_pars = {\n",
    "    \"n_estimators\" : np.linspace(10, 100, 4).astype(int),      # Number of trees\n",
    "    \"max_features\" : np.linspace(1, Ncols, 4).astype(int),     # Number of columns to pick for each tree\n",
    "    \"max_samples\" : np.linspace(0.1, 0.9, 4)       # Bootstrap set to True by default\n",
    "}\n",
    "#2. Create a GridSearchCV object, using the random forest regressor:\n",
    "grid_regression = GridSearchCV(RandomForestRegressor(), param_grid=grid_pars)\n",
    "\n",
    "#Note: Because there is so much training data, using the full dataset takes too long. So here we'll just use 10%\n",
    "np.random.seed(42)\n",
    "idx = np.sort(np.random.choice(len(X), size=int(len(X)*0.1), replace=False))\n",
    "#3. Fit to training data in (the subset of) X and y\n",
    "grid_regression.fit(X[idx,:],y[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HUuX3bOYcYIM"
   },
   "source": [
    "Here we print the best parameters from the grid search (on the training/validation cross-validation run):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "ShaXFzjQcYIN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best max_features : 8\n",
      "Best max_samples : 0.6333333333333333\n",
      "Best n_estimators : 40\n"
     ]
    }
   ],
   "source": [
    "# Print best values\n",
    "for key in grid_regression.best_params_:\n",
    "    print(f\"Best {key} : {grid_regression.best_params_[key]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dczjj2ercYIN"
   },
   "source": [
    "### Question 15: RMSE for the Random Forest Regressor [1 mark]\n",
    "\n",
    "Finally compute the RMSE for the training and test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "1SAtgw13cYIO"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31.366585598892463, 76.23356023972379)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create new Forest with best found parameters\n",
    "bestForest = RandomForestRegressor(**grid_regression.best_params_)\n",
    "\n",
    "# Run Random Forest\n",
    "bestForest.fit(X, y)\n",
    "\n",
    "# Make predictions\n",
    "ypred_rf_train = bestForest.predict(X)\n",
    "ypred_rf_test = bestForest.predict(Xtest)\n",
    "\n",
    "# Errors\n",
    "rmse_rf_train = RMSE(ypred_rf_train, y) \n",
    "rmse_rf_test = RMSE(ypred_rf_test, ytest)\n",
    "\n",
    "rmse_rf_train, rmse_rf_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best forest RMSE:\n",
      "train: 31.367, test: 76.234\n",
      "\n",
      "Grid regression RMSE:\n",
      "train: 64.083, test: 79.858\n"
     ]
    }
   ],
   "source": [
    "# Actually, I found out that using a brand new forest and\n",
    "# the predict() method of grid_regression object yields different results.\n",
    "# Not sure why this is, since from the documentation the grid_regression object\n",
    "# should use the same model and the same best found parameters.\n",
    "\n",
    "for obj, name in zip([bestForest, grid_regression], [\"Best forest\", \"Grid regression\"]):\n",
    "    train_err = RMSE(obj.predict(X), y)\n",
    "    test_err = RMSE(obj.predict(Xtest), ytest)\n",
    "    print(f\"\\n{name} RMSE:\\ntrain: {train_err:.3f}, test: {test_err:.3f}\")\n",
    "    \n",
    "# From these results, it seems that the best approach is the best forest one,\n",
    "# i.e. start with a new RandomForestRegressor() explicitly with the best parameters\n",
    "# from grid_regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S_j3g8cycYIP"
   },
   "source": [
    "We can compare this to the standard deviations for the two sets of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "FRjHEn1pcYIQ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(82.43136001277581, 103.25109103178704)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(y), np.std(ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(72.40451940025356, 87.60477726176146)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse_lasso_train, rmse_lasso_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wpY-t0TfcYIQ"
   },
   "source": [
    "### Question 16: Did the random forest do better than lasso regression? [1 mark]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "KOBVQyVpcYIR"
   },
   "outputs": [],
   "source": [
    "# The RMSE of the random forest (took the best forest method) is lower than \n",
    "# in the lasso regression, however this differnce is more noticeable \n",
    "# in the training set rather than in the test set.\n",
    "# Still, for the test set, the RMSE of the random forest is lower than lasso regression,\n",
    "# so random forest did perform better."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
